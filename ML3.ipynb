{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ASSIGNMENT\n",
        "\n",
        "QUES1.What is a Decision Tree, and how does it work in the context of classification?\n",
        "- A Decision Tree is a machine learning model that looks like a flowchart. It splits the data into smaller groups by asking yes/no type questions at each step. In classification, the tree checks features step by step until it reaches a final class at the leaf node. For example, to classify a flower, the tree may ask questions about petal length, petal width, etc., and finally decide which species it belongs to. It is simple to understand and easy to visualize.\n",
        "\n",
        "QUES2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "- Both Gini Impurity and Entropy measure how \"impure\" or mixed the classes are in a group.  \n",
        "- Gini Impurity shows the probability of wrongly classifying an item if we randomly assign a label. Lower Gini means purer groups.  \n",
        "- Entropy measures disorder or randomness in the data. If all items belong to one class, entropy is 0 (pure).  \n",
        "\n",
        "In a Decision Tree, the algorithm chooses splits that reduce impurity (low Gini or entropy). This way, each split makes the groups more homogenous and improves accuracy.\n",
        "\n",
        "QUES3.What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "- - Pre-Pruning means stopping the tree from growing too deep in the first place. For example, setting a maximum depth or minimum samples per split. Its main advantage is saving time and reducing overfitting.  \n",
        "- Post-Pruning means letting the tree grow fully and then cutting unnecessary branches after training. Its advantage is that it creates a simpler model while keeping the accuracy high.  \n",
        "\n",
        "Both methods help avoid overly complex trees and improve generalization on new data.\n",
        "\n",
        "QUES4.What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "- Information Gain measures how much impurity decreases after making a split. It compares the impurity before and after splitting the data. The higher the Information Gain, the better the split.  \n",
        "\n",
        "It is important because it guides the tree to choose the most useful feature at each step. This ensures that the tree becomes accurate, efficient, and easier to understand, instead of making random splits.\n",
        "\n",
        "QUES5.What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "- Applications of Decision Trees include:  \n",
        "- Medical diagnosis (predicting diseases)  \n",
        "- Banking and finance (loan approval, fraud detection)  \n",
        "- Marketing (customer churn prediction)  \n",
        "- E-commerce (product recommendations)  \n",
        "\n",
        "Advantages: They are easy to understand, can handle both numbers and categories, and require little data preparation.  \n",
        "Limitations: They can overfit the data, are sensitive to small changes, and sometimes create very large trees that are hard to manage.  \n"
      ],
      "metadata": {
        "id": "5K5QcSZaHj2o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaFjnXfqHcH9",
        "outputId": "1b419381-ecda-44a8-d7f8-a20d602b63eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.01667014 0.         0.40593501 0.57739485]\n"
          ]
        }
      ],
      "source": [
        "#QUES6.Write a Python program to:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model with Gini\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print accuracy and feature importance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUES7.Write a Python program to:\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Limited depth\n",
        "clf1 = DecisionTreeClassifier(max_depth=3)\n",
        "clf1.fit(X_train, y_train)\n",
        "\n",
        "# Full tree\n",
        "clf2 = DecisionTreeClassifier()\n",
        "clf2.fit(X_train, y_train)\n",
        "\n",
        "print(\"Max_depth=3 Accuracy:\", accuracy_score(y_test, clf1.predict(X_test)))\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, clf2.predict(X_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L7mT3A0IkJR",
        "outputId": "8c53c11d-f649-49fa-9d55-567b61473e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max_depth=3 Accuracy: 1.0\n",
            "Full Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUES8.Write a Python program to:\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train regressor\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "print(\"MSE:\", mean_squared_error(y_test, reg.predict(X_test)))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7W9JCJrIlmE",
        "outputId": "8a0bedd3-216d-48e7-8eb6-61fc30396f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5003812696454941\n",
            "Feature Importances: [0.52777883 0.05275811 0.0532922  0.02747635 0.03071986 0.13138811\n",
            " 0.09359448 0.08299207]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUES9.Write a Python program to:\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor # Changed from DecisionTreeClassifier\n",
        "\n",
        "# Parameters to tune\n",
        "params = {\"max_depth\":[2,3,4,None], \"min_samples_split\":[2,3,4]}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeRegressor(), params, cv=3) # Changed from DecisionTreeClassifier\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqg3cZCxJWkd",
        "outputId": "79ba76ed-e046-4731-a857-dd19d490335d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': None, 'min_samples_split': 4}\n",
            "Best Accuracy: 0.6052932867473401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUES10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process.\n",
        "- Step 1: Handle missing values → For numerical data, fill with average/median. For categorical data, fill with most frequent value.  \n",
        "- Step 2: Encode categorical features → Convert text into numbers using One-Hot Encoding or Label Encoding.  \n",
        "- Step 3: Train Decision Tree → Use the cleaned data to train a decision tree model.  \n",
        "- Step 4: Tune hyperparameters → Use GridSearchCV to try different max_depth, min_samples_split, etc., and select the best model.  \n",
        "- Step 5: Evaluate performance → Check accuracy, precision, recall, and confusion matrix on the test set.  \n",
        "\n",
        "Business value: This model can help doctors predict diseases quickly and accurately, reduce human error, save costs, and provide better healthcare services to patients.\n"
      ],
      "metadata": {
        "id": "ByPbqAYCJpnQ"
      }
    }
  ]
}